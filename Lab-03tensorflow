#시즌2 Lab-03 Linear Regression and How to Minimize cost

import numpy as np   # numpy를 import한다.

X = np.array([1, 2, 3]) # x입력값을 지정한다.
Y = np.array([1, 2, 3]) # y출력값을 지정한다.

def cost_func(W, X, Y):  # 데이터 x,y에 대해 w값이 주어졌을때 cost값을 계산하는 함수
  hypothesis = X * W     # hypothesis(예측값)은 x와 w를 곱한 값.
  return tf.reduce_mean(tf.square(hypothesis - Y)) #예측값-실제값(y)에 제곱을 하고 평균을 낸 값이 cost함수.

W_values = np.linspace(-3, 5, num=15) #numpy의 linspace를 이용해서 -3부터 5까지를 15개로 나눈다.
cost_values = []   #그 나눈값은 리스트로 받는다.

for feed_W in W_values:   #위에서 받은 리스트값을 각각 뽑아낸다.
    curr_cost = cost_func(feed_W, X, Y)   #뽑아내서 weight값으로 사용한다.
    cost_values.append(curr_cost)   #feed_w값에 따라 바뀌는 cost값의 변화를 저장하고 출력한다.
    print("{:6.3f} | {:10.5f}".format(feed_W, curr_cost))   #결과 출력.
    
#Gradient descent
tf.set_random_seed(0)  #random_seed를 특정한 값으로 초기화한다.
x_data = [1., 2., 3., 4.]  # x데이터를 지정한다.
y_data = [1., 3., 5., 7.]  # y데이터를 지정한다.

W = tf.Variable(tf.random_normal([1], -100., 100.))  #random_normal(정규분포)를 따르는 1개짜리 변수를 만들어 w에 할당한다.

for step in range(300):  #gradient descent부분을 300번 반복한다.
    hypothesis = W * X   #hypothesis는 wx로 정의.
    cost = tf.reduce_mean(tf.square(hypothesis - Y))  #cost값을 차이의 제곱의 평균으로 지정한다.

    alpha = 0.01  #alpha=0.01로 지정.
    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))
    descent = W - tf.multiply(alpha, gradient)  #gradient와 alpha값을 곱하고 W에서 빼줍니다.
    W.assign(descent)    #새로운 w값을 W에 할당하며 업데이트 한다.
    
    if step % 10 == 0:  #10번 반복 하면서 cost값과 w값을 출력한다.
        print('{:5} | {:10.4f} | {:10.6f}'.format(
            step, cost.numpy(), W.numpy()[0]))
